{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.data.dataset import SNLIVocabulary, SNLIDataset\n",
    "from src.models.net import NLIModel\n",
    "from src.models.classifiers import Classifier\n",
    "from src.models.encoders import BiLSTMEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = SNLIDataset(\"data/\", split=\"train\",).vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "hidden_size= 300\n",
    "\n",
    "encoder = BiLSTMEncoder(\n",
    "    word_embeddings=vocab.word_embedding,\n",
    "    input_size=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    max_pooling=True,\n",
    ")\n",
    "classifier = Classifier(2 * hidden_size)\n",
    "model = NLIModel(encoder, classifier).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/bilstm-max/best_model.pt\", map_location=device))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model: nn.Module,\n",
    "    vocab: SNLIVocabulary,\n",
    "    device: torch.device,\n",
    "    premise: str,\n",
    "    hypothesis: str,\n",
    ") -> str:\n",
    "    \"\"\"Predict the entailment label of the given premise and hypothesis.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model (encoder + classifier).\n",
    "        vocab (SNLIVocabulary): The vocabulary.\n",
    "        device (torch.device): The device to use.\n",
    "        premise (str): The premise.\n",
    "        hypothesis (str): The hypothesis.\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted entailment label.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    id_to_label = {\n",
    "            0: \"entailment\",\n",
    "            1: \"neutral\",\n",
    "            2: \"contradiction\",\n",
    "        }\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and index the premise and hypothesis\n",
    "        premise_indices = vocab.tokenize_and_index(premise)\n",
    "        hypothesis_indices = vocab.tokenize_and_index(hypothesis)\n",
    "\n",
    "        # Convert indices to tensors and wrap them in a list\n",
    "        premise_indices = [torch.tensor(premise_indices, dtype=torch.long)]\n",
    "        hypothesis_indices = [torch.tensor(hypothesis_indices, dtype=torch.long)]\n",
    "\n",
    "        # Pad sequences and compute lengths\n",
    "        padded_premises = pad_sequence(premise_indices, batch_first=True, padding_value=1)\n",
    "        premise_lengths = torch.tensor([len(premise_indices[0])], dtype=torch.long)\n",
    "        padded_hypotheses = pad_sequence(hypothesis_indices, batch_first=True, padding_value=1)\n",
    "        hypothesis_lengths = torch.tensor([len(hypothesis_indices[0])], dtype=torch.long)\n",
    "\n",
    "        # Move the batch to the device\n",
    "        padded_premises = padded_premises.to(device)\n",
    "        premise_lengths = premise_lengths.to(device)\n",
    "        padded_hypotheses = padded_hypotheses.to(device)\n",
    "        hypothesis_lengths = hypothesis_lengths.to(device)\n",
    "\n",
    "        # Compute the logits\n",
    "        logits = model(padded_premises, premise_lengths, padded_hypotheses, hypothesis_lengths)\n",
    "\n",
    "        # Get the predictions\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "    return id_to_label[int(predictions.item())]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Predictions using the BiLSTM model with max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: contradiction\n"
     ]
    }
   ],
   "source": [
    "premise_1 = \"Two men sitting in the sun\"\n",
    "hypothesis_1 = \"Nobody is sitting in the shade\"\n",
    "label_1 = predict(model, vocab, device, premise_1, hypothesis_1)\n",
    "print(f\"Label: {label_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: contradiction\n"
     ]
    }
   ],
   "source": [
    "premise_1 = \"A man is walking a dog\"\n",
    "hypothesis_2 = \"No cat is outside\"\n",
    "label_2 = predict(model, vocab, device, premise_1, hypothesis_2)\n",
    "print(f\"Label: {label_2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible reason for the failure is the presence of negations in the hypotheses, which might lead the model to focus on the opposite aspect between the premise and the hypothesis. The models may be more sensitive to negation words like \"nobody\" and \"no\" in the hypothesis, causing it to perceive a stronger contradiction than exists. Additionally, the model might struggle with understanding the relationships between different entities in the sentences, such as \"men\" and \"nobody,\" or \"dog\" and \"cat.\" This difficulty in capturing the semantic relationships between entities could lead the model to assess the relationship between the premise and the hypothesis incorrectly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following table shows the results of the models on the SNLI dev and test sets, and the micro and macro averaged results on the SentEval tasks.\n",
    "\n",
    "| **Model** | **SNLI Dev** | **SNLI Test** | **Micro** | **Macro** |\n",
    "|---|---|---|---|---|\n",
    "| Baseline | 0.657 | 0.654 | 80.498 | 79.155 |\n",
    "| LSTM | 0.815 | 0.814 | 76.088 | 75.567 |\n",
    "| BiLSTM | 0.807 | 0.816 | 79.128 | 78.634 |\n",
    "| BiLSTM (max) | 0.847 | 0.841 | 79.583 | 79.006 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
