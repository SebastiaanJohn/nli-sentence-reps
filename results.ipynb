{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from src.data.dataset import build_vocabulary, tokenize, tokens_to_indices\n",
    "from src.models.net import NLIModel\n",
    "from src.models.classifiers import Classifier\n",
    "from src.models.encoders import BiLSTMEncoder\n",
    "\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the vocabulary and the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx, word_embeddings = build_vocabulary(split=\"train\", glove_version=\"840B\", word_embedding_dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BiLSTMEncoder(\n",
    "    word_embeddings=word_embeddings,\n",
    "    input_dim=300,\n",
    "    output_dim=2048,\n",
    "    max_pooling=True,\n",
    ")\n",
    "classifier = Classifier(input_dim=4096, num_classes=3)\n",
    "model = NLIModel(encoder, classifier).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/bilstm-max/best_model.pt\", map_location=device))\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a prediction function that takes a sentence and returns the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model: nn.Module,\n",
    "    token_to_idx: dict[str, int],\n",
    "    device: torch.device,\n",
    "    premise: str,\n",
    "    hypothesis: str,\n",
    ") -> str:\n",
    "    \"\"\"Predict the entailment label of the given premise and hypothesis.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model (encoder + classifier).\n",
    "        token_to_idx (dict[str, int]): The token to index mapping.\n",
    "        device (torch.device): The device to use.\n",
    "        premise (str): The premise.\n",
    "        hypothesis (str): The hypothesis.\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted entailment label.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    id_to_label = {\n",
    "            0: \"entailment\",\n",
    "            1: \"neutral\",\n",
    "            2: \"contradiction\",\n",
    "        }\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the premise and hypothesis\n",
    "        premise_tokens = tokenize(premise)\n",
    "        hypothesis_tokens = tokenize(hypothesis)\n",
    "        \n",
    "        # Convert tokens to indices\n",
    "        premise_indices = tokens_to_indices(premise_tokens, token_to_idx)\n",
    "        hypothesis_indices = tokens_to_indices(hypothesis_tokens, token_to_idx)\n",
    "\n",
    "        # Convert indices to tensors and wrap them in a list\n",
    "        premise_indices = [torch.tensor(premise_indices, dtype=torch.long)]\n",
    "        hypothesis_indices = [torch.tensor(hypothesis_indices, dtype=torch.long)]\n",
    "\n",
    "        # Pad sequences and compute lengths\n",
    "        padded_premises = pad_sequence(premise_indices, batch_first=True, padding_value=1)\n",
    "        premise_lengths = torch.tensor([len(premise_indices[0])], dtype=torch.long)\n",
    "        padded_hypotheses = pad_sequence(hypothesis_indices, batch_first=True, padding_value=1)\n",
    "        hypothesis_lengths = torch.tensor([len(hypothesis_indices[0])], dtype=torch.long)\n",
    "\n",
    "        # Move the batch to the device\n",
    "        padded_premises = padded_premises.to(device)\n",
    "        premise_lengths = premise_lengths.to(device)\n",
    "        padded_hypotheses = padded_hypotheses.to(device)\n",
    "        hypothesis_lengths = hypothesis_lengths.to(device)\n",
    "\n",
    "        # Compute the logits\n",
    "        logits = model(padded_premises, premise_lengths, padded_hypotheses, hypothesis_lengths)\n",
    "\n",
    "        # Get the predictions\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "    return id_to_label[int(predictions.item())]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Predictions using the BiLSTM model with max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 'contradiction', correct label: 'Neutral'\n"
     ]
    }
   ],
   "source": [
    "premise_1 = \"Two men sitting in the sun\"\n",
    "hypothesis_1 = \"Nobody is sitting in the shade\"\n",
    "label_1 = predict(model, token_to_idx, device, premise_1, hypothesis_1)\n",
    "print(f\"Predicted label: '{label_1}', correct label: 'Neutral'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 'contradiction', correct label: 'Neutral'\n"
     ]
    }
   ],
   "source": [
    "premise_2 = \"A man is walking a dog\"\n",
    "hypothesis_2 = \"No cat is outside\"\n",
    "label_2 = predict(model, token_to_idx, device, premise_2, hypothesis_2)\n",
    "print(f\"Predicted label: '{label_2}', correct label: 'Neutral'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible reason for the failure is the presence of negations in the hypotheses, which might lead the model to focus on the opposite aspect between the premise and the hypothesis. The models may be more sensitive to negation words like \"nobody\" and \"no\" in the hypothesis, causing it to perceive a stronger contradiction than exists. Additionally, the model might struggle with understanding the relationships between different entities in the sentences, such as \"men\" and \"nobody,\" or \"dog\" and \"cat.\" This difficulty in capturing the semantic relationships between entities could lead the model to assess the relationship between the premise and the hypothesis incorrectly. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The following table shows the results of the models on the SNLI dev and test sets, and the micro and macro averaged results on the SentEval tasks.\n",
    "\n",
    "| **Model** | **SNLI Dev** | **SNLI Test** | **Micro** | **Macro** |\n",
    "|---|---|---|---|---|\n",
    "| Baseline | 0.671 | 0.672 | 80.611 | 79.123 |\n",
    "| LSTM | 0.800 | 0.799 | 76.843 | 76.280 |\n",
    "| BiLSTM | 0.795 | 0.796 | 80.516 | 80.019 |\n",
    "| BiLSTM (max) | 0.836 | 0.836 | 82.556 | 81.764 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance\n",
    "\n",
    "BiLSTM (max) outperforms the other models on SNLI and SentEval tasks. This can be attributed to the architecture of the BiLSTM with max pooling. The bidirectional LSTM captures information from both forward and backward directions, which helps the model learn more contextualized sentence representations. Max pooling allows the model to focus on the most salient features of the input sequence, making it more robust to variations in sentence length and structure. However, the baseline model, which only averages word embeddings, performs worse than BiLSTM and BiLSTM (max) but better than the unidirectional LSTM. This suggests that while the baseline model is simplistic, it can still capture helpful information about the sentences. The unidirectional LSTM has a lower performance, possibly because it only captures information in one direction, limiting its ability to understand complex sentence structures.\n",
    "\n",
    "### Model failures\n",
    "\n",
    "All models can fail when facing complex sentence structures, negations, or dependencies that require a deeper understanding of the language. The baseline model will likely struggle more in these cases, as it relies solely on the average word embeddings and needs more information about word order and context. The unidirectional LSTM may also need help capturing dependencies that require considering the context from both directions.\n",
    "The BiLSTM and BiLSTM (max) models should be more robust to such issues due to their bidirectional nature, but they are imperfect. They can fail when long-range dependencies or semantic relationships between premises and hypotheses are intricate.\n",
    "\n",
    "### Sentence representations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence embeddings represent a fixed-size vector representation that aims to capture the meaning and structure of a sentence. In the baseline model, the sentence embeddings mainly represent the average of the words' meanings, which may lose information about word order and context. The LSTM, BiLSTM, and BiLSTM (max) models can better capture the sequential nature of sentences and the context in which words appear. However, even the more complex models might lose some information. For example, they might struggle with capturing the nuances of certain syntactic or semantic relationships (such as in the example above)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional experiment\n",
    "### Research question: How do the models perform on sentences with different lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from src.data.dataset import get_dataset, snli_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_length_info(example: dict) -> dict:\n",
    "    \"\"\"Add the combined length of premise and hypothesis to the example.\n",
    "\n",
    "    Args:\n",
    "        example (dict): An example from the dataset\n",
    "\n",
    "    Returns:\n",
    "        dict: The example with an additional \"length\" field\n",
    "    \"\"\"\n",
    "    example[\"length\"] = len(example[\"premise\"]) + len(example[\"hypothesis\"])\n",
    "    return example\n",
    "\n",
    "def split_dataset_by_quantiles(dataset: Dataset, quantiles: list[float]) -> list[Dataset]:\n",
    "    \"\"\"Split the dataset into subsets based on sentence length quantiles.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to split\n",
    "        quantiles (List[float]): A list of quantiles to use as the split points\n",
    "\n",
    "    Returns:\n",
    "        List[Dataset]: A list of datasets containing examples grouped by sentence length\n",
    "    \"\"\"\n",
    "    # Add length information to the dataset\n",
    "    dataset = dataset.map(add_length_info)\n",
    "\n",
    "    # Calculate the quantiles\n",
    "    lengths = dataset[\"length\"]\n",
    "    quantile_values = np.quantile(lengths, quantiles)\n",
    "\n",
    "    # Create a list of datasets by filtering based on the quantile values\n",
    "    datasets = []\n",
    "    for i, q in enumerate(quantile_values):\n",
    "        if i == 0:\n",
    "            lower_bound = 0\n",
    "        else:\n",
    "            lower_bound = quantile_values[i - 1]\n",
    "        upper_bound = q\n",
    "        subset = dataset.filter(lambda example: lower_bound <= example[\"length\"] < upper_bound)\n",
    "        datasets.append(subset)\n",
    "\n",
    "    # Add the last subset containing examples with lengths greater than or equal to the last quantile value\n",
    "    datasets.append(dataset.filter(lambda example: example[\"length\"] >= quantile_values[-1]))\n",
    "\n",
    "    # Remove the \"length\" field from the datasets\n",
    "    for subset in datasets:\n",
    "        subset = subset.remove_columns([\"length\"])\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    eval_data: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Evaluate the model on the given data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model (encoder + classifier).\n",
    "        criterion (nn.Module): The loss function.\n",
    "        eval_data (DataLoader): The evaluation data.\n",
    "        device (torch.device): The device to use.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: The average loss and accuracy on the evaluation data.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Keep track of the evaluation loss and correct predictions\n",
    "    eval_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_data, desc=\"Evaluating\"):\n",
    "\n",
    "            # Unpack the batch\n",
    "            (premise, hypothesis,\n",
    "             premise_lengths, hypothesis_lengths,\n",
    "             label) = batch\n",
    "\n",
    "            # Move the batch to the device\n",
    "            premise = premise.to(device)\n",
    "            premise_lengths = premise_lengths.to(device)\n",
    "            hypothesis = hypothesis.to(device)\n",
    "            hypothesis_lengths = hypothesis_lengths.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Compute the logits\n",
    "            logits = model(premise, premise_lengths, hypothesis, hypothesis_lengths)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(logits, label)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            # Get the predictions\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Count the correct predictions\n",
    "            correct_predictions += (predictions == label).sum().item()\n",
    "\n",
    "        # Compute the average evaluation loss\n",
    "        eval_loss = eval_loss / len(eval_data)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        accuracy = correct_predictions / len(eval_data.dataset)\n",
    "\n",
    "    return eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short dataset size: 2843\n",
      "Medium dataset size: 3543\n",
      "Long dataset size: 3438\n"
     ]
    }
   ],
   "source": [
    "# Get the test dataset\n",
    "test_dataset = get_dataset(\"test\")\n",
    "\n",
    "# Split the dataset into subsets based on sentence length quantiles\n",
    "short_dataset, medium_dataset, long_dataset = split_dataset_by_quantiles(test_dataset, quantiles=[0.33, 0.66])\n",
    "\n",
    "print(f\"Short dataset size: {len(short_dataset)}\")\n",
    "print(f\"Medium dataset size: {len(medium_dataset)}\")\n",
    "print(f\"Long dataset size: {len(long_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 15.56\n",
      "Average sentence length: 21.77\n",
      "Average sentence length: 32.26\n"
     ]
    }
   ],
   "source": [
    "# Get the average sentence length for each dataset\n",
    "for dataset in [short_dataset, medium_dataset, long_dataset]:\n",
    "    print(f\"Average sentence length: {np.mean(dataset['length']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the Short dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 89/89 [00:05<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short dataset evaluation loss: 0.4065, accuracy: 0.8505\n",
      "Evaluating the model on the Medium dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 111/111 [00:08<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium dataset evaluation loss: 0.4432, accuracy: 0.8335\n",
      "Evaluating the model on the Long dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 108/108 [00:12<00:00,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long dataset evaluation loss: 0.4704, accuracy: 0.8255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "for name, split in zip([\"Short\", \"Medium\", \"Long\"], [short_dataset, medium_dataset, long_dataset], strict=True):\n",
    "    # Create a dataloader for the dataset\n",
    "    eval_data = DataLoader(split, batch_size=32, collate_fn=partial(snli_collate_fn, token_to_idx))\n",
    "\n",
    "    # Evaluate the model on the dataset\n",
    "    print(f\"Evaluating the model on the {name} dataset...\")\n",
    "    eval_loss, accuracy = evaluate(model, criterion, eval_data, device)\n",
    "\n",
    "    print(f\"{name} dataset evaluation loss: {eval_loss:.4f}, accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
